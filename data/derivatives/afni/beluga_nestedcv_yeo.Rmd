---
title: "Beluga: Analysis of Functional Connectivity With Yeo (2011) Network Parcels"
output:
  html_document:
    code_folding: hide
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(magrittr)
library(ggplot2)
library(ggthemes)
library(ppcor)
library(reshape2)
library(glmnet)
library(ggseg)
library(ggsegYeo2011)
library(kableExtra)
library(xtable)
library(broom)
library(ggsci)
library(nestedcv)
library(car)
library(ggrepel)
```

# Rationale

despite considerable progress in memory research, we still struggle to understand how and why memories are lost and forgotten.
In fact, researchers disagree even about when and how forgetting takes place. Consider, for example, the three-stage model of a memory’s lifecycle, which is often used in cognitive research. In this idealized depiction, memories are first encoded as engrams in the hippocampus (synaptic consolidation); then undergo a period of consolidation (system consolidation); and they are finally retrieved at some later time—which can vary from minutes to years. Because forgetting is only inferred after a failed retrieval attempt, it is difficult to pinpoint at which of the earlier stages it really occurred. Specifically, researchers disagree on whether forgetting is a process that is best thought of as occurring during the consolidation or retrieval stages. 

According to the __storage decay__ class of theories, forgetting is a process that mostly affects consolidation. In this view, engrams are intrinsically labile and accumulate progressive damage over time. Such damage could be due, for example, to biological decay processes (Davis & Zhong, 2017; Hardt et al., 2013). It could also be due to the engram’s synapses being partially overwritten by the encoding of additional traces (a form of catastrophic interference). Finally, it has been suggested that spontaneous brain activity might play a pivotal role in consolidating memories (Pezzulo et al., 2021), and that its disruption might underpin abnormal forgetting in amnestic neurodegeneration (Jin et al., 2012; Qi et al., 2018). 

According to the __retrieval failure__ class of theories, engrams are relatively stable over long intervals, and forgetting is instead driven by an inability to access and retrieve them. As in the case of storage decay, multiple processes have been hypothesized to explain retrieval failure. For example, memories might become less accessible because of the accumulation of more memories that compete for retrieval (retrieval interference). Since controlled memory retrieval is underpinned by the prefrontal cortex, it is possible that loss of connections between prefrontal cortices and the hippocampus render some memories inaccessible. Evidence for these theories comes from the fact that direct neurostimulation can re-awaken long-forgotten memories, a fact that has been reported in humans (Penfield, 1968) and, more recently, in animal models of Alzheimer’s Disease (Roy et al., 2016)

## Implications and Predictions of Storage Decay vs. Retrieval Failure 

The distinction between these two families of theories is not only academic, and it has important implications for memory research and for clinical practice. Consider, for example, the dramatic form of abnormal forgetting that is Alzheimer’s Disease (AD). If the storage decay theory is correct, the promising new clinical treatments that remove amyloid beta deposits (e.g., (Cummings et al., 2023; Swanson et al., 2021)) would stop the progression of the disease but would not restore memory function and would not bring back lost memories, as the underlying neural substrate has already been compromised. But according to the retrieval failure hypothesis, at least some lost memories might be retrievable, and function would be at least partially restored. Knowing which theory is correct, therefore, is important to guide treatment and future clinical research. 

Fortunately, these two theories can be empirically tested because they make different predictions. According to the storage decay framework, forgetting should be associated with reduced gray matter densities in regions devoted to engram encoding, such as the hippocampus and the parahippocampal cortex, or with reduced the functional connectivity between regions involved in memory encoding and consolidation, such as those belonging to the Default Mode Network (DMN: (Kaefer et al., 2022; Raichle, 2015)), or between DMN regions and cortical association areas, where long-term memories might be stored as the end-product of consolidation (Alvarez & Squire, 1994; McClelland et al., 1995; O’Reilly et al., 2014).
According to the retrieval failure framework, however, forgetting occurs because of an inability to properly access previously stored memories. Because the retrieval process is controlled by prefrontal cortices, the theory predicts that greater forgetting should be associated with anatomical differences in regions related to executive control, most notably the lateral and medial prefrontal cortices (Badre & Wagner, 2007; Lepage et al., 2000; Petrides, 2002; Thompson-Schill et al., 1997). At the level of functional connectivity, it similarly predicts that greater forgetting should be associated with reduced functional connectivity between prefrontal regions and memory encoding regions.

## Obstacles to the Testing the Theories 

Because the two theories make different predictions, it should be possible to empirically test them by examining which features of individual anatomy and functional connectivity co-vary with individual differences in forgetting. One important obstacle, however, remains: it is extremely difficult to quantify forgetting in the first place. As noted above, forgetting is only observed when participants fail to correctly recall or recognize an item in a memory test. Because forgetting itself is unobservable, performance failure is the only ground truth available. Unfortunately, this carries significant limitations. First, common behavioral measures such as the proportion of correctly remembered items are prone to statistical misinterpretations (Brady et al., 2023; Loftus, 1978). Second, performance in memory tests is also affected by the specifics of the paradigms used. For example, memory performance differs greatly when the same materials are tested using recognition or recall; recognition is generally easier, but recall, being more effortful, leads to better consolidation and retention. Finally, most laboratory tests fail to capture the temporal dynamics of forgetting. Forgetting is a process, but it is tested only at a specific snapshot in time, and there is no consensus on how long after encoding a memory should be tested. Because long-term memory retention which spans from seconds to decades, it is difficult to test it in a laboratory setting. Consequently, previous studies have relied on individuals exhibiting dramatic differences in memory function, such as amnesic patients (such as patients suffering from medial temporal lobe injury: (Corkin, 2002; Gabrieli et al., 1988; Scoville & Milner, 1957)). Conversely, most laboratory experiments focus on relatively short retention intervals. Two participants, however, might have comparable memory performance a few minutes after studying but vastly different performances weeks later.
The goal of this study is to overcome these limitations by using an innovative, model-based approach to formally quantify individual differences in forgetting in a sample of healthy individuals for whom functional connectivity data are available. Analyzing which connectivity features predict individual differences in forgetting would empirically test  storage decay and retrieval failure theories.


# Analysis of Functional Connectivity in Beluga

To calculate functional connectivity, each participant's brain was divided into discrete regions using the network parcellation originally proposed by Yeo et al (Yeo et al. 2011). This parcellation scheme divides the brain into 17 networks, each of which is thought to be involved in a different aspect of cognition. The networks are named as follows: Visual A, Visual B, Somatomotor A, Somatomotor B, Dorsal Attention, Ventral Attention, Limbic, Frontoparietal, Default A, Default B, Default C, Temporal Parietal, Temporal Occipital, Cingulo Opercular, Cingulo Parietal, Salience, and Subcortical.

This information is stored in the `yeo.csv` file.

```{r}
yeo17networks <- read_csv("yeo17.csv", show_col_types = F)
networks <- yeo17networks$region[2:18]
```

From this data, we need to extract the color and name information. 

```{r, fig.width=7, fig.height=3.5}
yeo17colors <- rgb(yeo17networks[2:18,2:4], 
                   maxColorValue = 255)

# This is needed to make sure the colors are assigned to the right networks
names(yeo17colors) <- networks 
```

And, finally, we can visualize the Yeo parcellation using the original color 
scheme.

```{r, fig.width=7, fig.height=3.5}
# Visualize the Yeo parcellation
ggplot(tibble(region=networks), aes(fill=region)) +
  geom_brain(atlas = ggsegYeo2011::yeo17, 
             position = position_brain("horizontal"),
             color="white") + 
  scale_fill_manual(values = yeo17colors,
                    na.value = "black") +
  labs(fill = "") +
  ggtitle("Yeo et al. (2011) 17-Network Parcellation") +
  theme_void()+
  theme(legend.position = "bottom") +
  guides(fill=guide_legend(ncol=4))
  
ggsave("figures/yeo17.png")
```

# Data Preparation

## Extracting Individual Connectomes

Each participant’s global pattern of brain connectivity, known as the connectome (Sporns, 2011), was extracted using a standard procedure (Cole et al., 2016; Shen et al., 2017). For each of the 17x17 pairs, the Pearson correlation coefficient between the respective timeseries was computed. 

In fact, we will create _two_ such connectomes: one using standard Pearson 
correlation coefficients (matrix `R`) and one using partial correlation 
coefficients (matrix `PR`), where the correlations between networks are computed
after partialling out their correlations with every other network. 

This step needs to be done only once, and can be skipped by setting the 
`CREATE_FC`  variable to `F`. 

```{r}
CREATE_FC = F

if (CREATE_FC) {
  for (sub in dir()[grep("sub-*", dir())]) {
    netdata <- NULL
    for (network in 1:17) {
      file_name <- paste(sub,
                         "_yeo17_network",
                         sprintf("%02d", network),  
                         "_ss.txt",
                         sep="")
      #mat <- t(read.table(paste(sub, "func", file_name, sep="/")))
      #pc1 <- prcomp(mat)  # PCA
      #pc1 <- pc1$x[,1]    # first PC
      mat <- colMeans(read.table(paste(sub, "func", file_name, sep="/")))
      pc1 <- mat
      table <- tibble(subject = sub,
                      scan = 1:210,
                      timeseries = pc1,
                      network = network)
      if (is.null(netdata)) {
        netdata <- table
      } else {
        netdata %<>% bind_rows(table)
      }
    }
    
    # Pivot long data format into wide data 
    wnetdata <- netdata %>% pivot_wider(id_cols = scan, 
                                        names_from = network, 
                                        values_from = timeseries)
    X  <- as.matrix(wnetdata[,2:18])/1000 
    PR <- pcor(X)$estimate
    R  <- cor(X)
    
    # Generate matrices:
    
    # The partial correlation matrix
    long_pr <- melt(PR)
    #pdf(paste(sub, "fc_pcorr.pdf", sep="/"))
    ggplot(long_pr, aes(x=Var1, y=Var2)) +
      geom_tile(aes(fill=value)) +
      coord_equal() +
      scale_fill_gradient2(limits=c(-1,1), 
                           low = "blue", 
                           high = "red", 
                           mid = "white") +
      theme_pander() +
      ggtitle(paste(sub, ": Functional Connectivity (Partial Correlations)", sep="")) +
      xlab("Networks") +
      ylab("Networks") 
    #dev.off()
    ggsave(paste(sub, "fc_pcorr.pdf", sep="/"),
           width=6, height=5)
    write.table(PR, col.names = T, 
                row.names = T, 
                file = paste(sub, "PR.txt", sep="/"))
    
    # The standard correlation matrix
    long_r <- melt(R)
    #pdf(paste(sub, "fc_corr.pdf", sep="/"))
    ggplot(long_r, aes(x=Var1, y=Var2)) +
      geom_tile(aes(fill=value)) +
      coord_equal() +
      scale_fill_gradient2(limits=c(-1,1), low = "blue", high = "red", mid = "white") +
      theme_pander() +
      ggtitle(paste(sub, ": Functional Connectivity, (Standard Correlations)", sep="")) +
      xlab("Networks") +
      ylab("Networks") 
    #dev.off()
    ggsave(paste(sub, "fc_corr.pdf", sep="/"),
           width=6, height=5)
    write.table(R, col.names = T, 
                row.names = T, 
                file = paste(sub, "R.txt", sep="/"))
  }
}
```

## Checking patterns of connectivity

The group-average of the resulting matrices shows a canonical distribution of r values, including the zones of maximum r values along the diagonal (corresponding to connectivity within the same networks). This pattern is entirely consistent with previous findings using the same parcellation scheme.

One problem with this canonical connectome is that the raw correlation coefficients are almost all positive and partially driven by common, correlated factors. To remove these confounds, we followed the procedure of Cole et al. (2016), and recalculated the correlation matrices using partial correlation coefficients, so that, when computing the correlation between two networks, the correlations between each network in the pair and the remaining 15 networks were partialled out. The resulting average connectome is sparser and includes both negative and positive correlations (Cole et al., 2016; Fox et al., 2005), giving a clearer picture of the underlying functional organization.


```{r}
## Load each subject's PR matrix and average them together after removing the diagonal and Z-transforming all the values
connectome <- NULL

for (sub in dir()[grep("sub-*", dir())]) {
  P <- read.table(paste(sub, "PR.txt", sep="/"))
  P <- cbind(networks, P)
  colnames(P) <- c("From", networks)
  wpconn <- as_tibble(P)
  lpconn <- wpconn %>% pivot_longer(cols = -From, names_to = "To", values_to = "Value") %>%
    add_column(Participant = sub, Correlation = "Partial Correlation")
  
  R <- read.table(paste(sub, "R.txt", sep="/"))
  R <- cbind(networks, R)
  colnames(R) <- c("From", networks)
  wrconn <- as_tibble(R)
  lrconn <- wrconn %>% pivot_longer(cols = -From, names_to = "To", values_to = "Value") %>%
    add_column(Participant = sub, Correlation = "Pearson Correlation") 
  
  
  conn <- bind_rows(lpconn, lrconn)
  
  if (is.null(connectome)) {
    connectome <- conn
  } else {
    connectome %<>% bind_rows(conn)
  }
}

connectome %>% mutate(Z = atanh(Value)) %>% 
  group_by(From, To, Correlation) %>% 
  summarize(Z=mean(Z)) %>%
  mutate(R = tanh(Z)) -> mean_connectome

mean_connectome$Correlation <- factor(mean_connectome$Correlation, 
                                      levels=c("Pearson Correlation", "Partial Correlation"))

# Plot mean connectome values as a tile plot, with the diagonal removed
ggplot(mean_connectome, aes(x=From, y=To)) +
  geom_tile(aes(fill=R), color="white") +
  scale_fill_gradient2(limits=c(-1, 1), low = "blue", high = "red", mid = "white", na.value="grey") +
  ggtitle("Mean Functional Connectivity") +
  coord_equal() +
  facet_wrap(~Correlation) +
  xlab("") +
  ylab("") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        axis.text.y = element_text(angle = 0, hjust = 1))

ggsave("figures/mean_connectome.png")
```

## Load the group-level data _X_

We now need to load the group level data. To apply ML methods, we need the data 
from all participants to be put into a matrix _X_ in which every individual is a 
row and every columns is a different Network-to-Network connection.

Because we have a small number of participants for whom we need the speed of 
forgetting, we need to reduce the connectivity matrix. For this reason, we will 
focus on the three networks that make up the Default Mode Network in Yeo (2011), 
which are named Default A, B, and C, and their connectivity with any of the 
other networks. This will results in 3 x 16 - 3 = 45 regressors.

Here are the three networks.

```{r, fig.width=4, fig.height=3}
## Creates a tibble where every regions is marked as NA 

defaults <- tibble(region = networks,
                   value = NA)

## Change the value of three networks that we are interested in; their values
## are set to the network name.

defaults$value[defaults$region == "Default A"] <- "Default A"
defaults$value[defaults$region == "Default B"] <- "Default B"
defaults$value[defaults$region == "Default C"] <- "Default C"

## Visualization of the three networks

## Add scale_color_viridis with discrete values

ggplot(defaults, aes(fill=value)) +
  geom_brain(atlas = ggsegYeo2011::yeo17, 
             position = position_brain(hemi ~ side),
             color="lightgrey") +
  #scale_fill_brewer(type = "qual", palette = "Accent", na.value="white") +
  scale_fill_d3() +
  labs(fill = "Network") +
  ggtitle("Default Mode Networks in Yeo et al. (2011)") +
  theme_void() 

ggsave("figures/dmn-networks.png")
```

And here is how we create the regressor matrix _X_.

```{r}
NOFLY <- c()
cols <- outer(1:17, 1:17, function(x, y) {paste(x, y, sep="-")})
cols %<>% as.vector

connection <- function(x, y) {
  paste(min(x, y), max(x, y), sep="-")
}

vconnection <- Vectorize(connection)

Mode <- function(x, na.rm=F) {
  if (na.rm) {
    x = x[!is.na(x)]
  }
  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}

k <- 3 * 16 - 3
n <- length(grep("sub-*", dir()))
X <- matrix(data = rep(0, k*n), nrow =  n)

j <- 1

for (sub in dir()[grep("sub-*", dir())]) {
  M <- read.table(paste(sub, "PR.txt", sep="/"))
  M[upper.tri(M, diag = T)] <- NA
  M <- M[15:17,]
  v <- M[!is.na(M)]
  
  X[j,] <- v
  if (length(v[is.na(v)]) > 0) {
    print(paste("NA detected in sub", sub))
    NOFLY %<>% c(sub)  # Adds sub to NOFLY list
  }
  j <- j + 1
}
```


## Load the dependent variable _Y_: The Speed of Forgetting

Now we need to load the dependent variable _Y_. In this case, the variable is 
the speed of forgetting $\phi$, which is stored as part of the participants' 
meta-data in `participats.tsv`. 

Individual values for the SOF parameter were calculated for each individual by averaging across the SOF values of each word pair they studied. 

Note that we could not measure $\phi$ for all participants, so we have to keep
track of which rows in the $X$ regressor matrix we want to exclude from our 
LASSO analysis.

```{r}
dvs <- read.table("participants.tsv", sep="\t", header=T)
keep <- !is.na(dvs$Alpha)  & !dvs$Subject %in% NOFLY
Y <- dvs$Alpha[keep]
X <- X[keep,]
```

The resulting values ranged from `r min(Y)` to `r max(Y)`, with a mean of `r mean(Y)` and standard deviation of `sd(Y)`. Both the mean and the range of values are in line with previously published studies (Sense et al., 2016; Zhou et al., 2021). And this is what the distribution of SOF values looks like:

```{r, fig.width=4, fig.height=4}
ggplot(dvs[keep,], aes(x=Alpha)) +
  geom_histogram(binwidth=0.02, fill="darkgrey", alpha=0.75, color="white") +
  ggtitle("Distribution of Speed of Forgetting") +
  ylab("Frequency") +
  
  # Add a vertical line with the mean value of SOF
  geom_vline(xintercept=mean(dvs$Alpha[keep]), 
             color="blue", linetype=1) +
  annotate("label", x=0.34, y=10, 
           color="blue",
           label = paste("Mean SOF =",
                         round(mean(dvs$Alpha[keep]), 3))) +
  xlab("Speed of Forgetting") +
  theme_minimal()

ggsave("figures/sof_distribution.png")
```

# LASSO Analysis with Nested CV


```{r}
# Nested CGV with glmnet  
set.seed(1234)
ncv.fit <- nestcv.glmnet(y=Y, x=atanh(X), 
                         family = "gaussian", 
                         alphaSet = 10/10,
                         type.measure="mae",
                         standardize=T,
                         grouped=F,
                         nlambdas = 100,
                         n_outer_folds = 5,
                         n_inner_folds = 5,
                         finalCV = T)
#ncv.fit

out <- ncv.fit$output
ggplot(out, aes(x=testy, y=predy)) +
  geom_point() +
  ggtitle("Nested Cross-Validation") +
  theme_minimal()
```

That is not great, but we need to consider the full model:

```{r, fig.width=4, fig.height=4}

# Create a tibble with the original values Y and the predictions from the ncv.fit object
yyhat <- tibble(y = Y, yhat = predict(ncv.fit$final_fit, X, s="lambda.min"))
ggplot(yyhat, aes(x=y, y=yhat)) +
  geom_point(size=3, color="darkgrey", alpha=0.75) +
  
  # Add diagonal identity line, grey and dashed 
  
  geom_abline(slope=1, intercept=0, col="black", linetype="dashed") +
  stat_smooth(method="lm", col="blue", fill="blue") +
  
  # Make sure botgh axies have same scale and limits
  coord_fixed(ratio=1, xlim=c(0.25, 0.38), ylim=c(0.25, 0.38)) +
  
  # add annotation with correlation value
  annotate("text", x=0.3, y=0.325, 
           label = paste("r(33) =",
                         round(cor(yyhat$y, yyhat$yhat), 2))) +
  
  # change x anbd y labels to observed and predicted
  xlab("Observed Speed of Forgetting") +
  ylab("Predicted Speed of Forgetting") +
  
  # add title predicted vs observed \phi
  ggtitle("Model Predictions vs. Data") +
  theme_minimal()

ggsave("figures/predicted_vs_observed.png")
```


## Predictive Connectivity 

First, we need to properly name the connections we are analyzing. 

```{r}
connections <- outer(networks, 
                     networks, 
                     function(a, b) {paste(a, "to", b)})

connections[upper.tri(connections, diag=T)] <- NA
restricted <- connections[15:17,]
restricted <- restricted[!is.na(restricted)]

```

Then, we filter out those that were set to zero by Lasso.

```{r}
# Get the list of non-zero regressors in Lasso

finalcoeff=ncv.fit$final_coef

betas <- as_tibble(finalcoeff) %>%
  add_column(code = rownames(finalcoeff))%>%
  filter(!is.na(meanExp)) 

brain <- tibble(connections = restricted,
                from = substring(restricted, 1, 9),
                region = substring(restricted, 14),
                #beta = betas[2:46])
                code = paste("V", 1:45, sep=""))


brain <- inner_join(betas, brain)

```

And now, we can visualize the table:

```{r}
brain %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

## Validation of the Model

Given equally valid predictors, the penalty term might sometimes force the LASSO algorithm to select one of them at random, providing an inadequate and partial picture of which regressors are indeed essential. If this were the case, other equally valid predictors would have been overlooked. A simple test to exclude this possibility  is to remove the 12 connections from the original matrix of 45 regressors X, and re-run the cross-validation procedure. If the procedure converges on a different and equally successful solution, then other important regressors were accidentally excluded.

When applied to our data, this test shows that the nested cross-validation procedure does not converge anymore. Specifically, it selects a larger hyperparameter ($\lambda$ = 0.0103) and fails to identify any consensus regressors; under these conditions, the resulting linear model reduces to the intercept and fails to capture any individual differences in forgetting.


```{r, fig.width=4, fig.height=4}
colnames(X) <- paste("V", 1:45, sep="")
selected <-rownames(ncv.fit$final_coef)[2:13]
Xprime <- X[,!colnames(X) %in% selected]

# Nested CGV with glmnet  
set.seed(1234)
ncv.fit2 <- nestcv.glmnet(y=Y, x=atanh(Xprime), 
                         family = "gaussian", 
                         alphaSet = 10/10,
                         type.measure="mae",
                         standardize=T,
                         grouped=F,
                         nlambdas = 100,
                         n_outer_folds = 5,
                         n_inner_folds = 5,
                         finalCV = T)
ncv.fit2

out2 <- ncv.fit2$output
ggplot(out2, aes(x=testy, y=predy)) +
  geom_point() +
  ggtitle("Nested Cross-Validation") +
  theme_minimal()
```

A second test consists of measuring the collinearity of the resulting model. Since the number of predictors in the model (_p_ = 12) is comparable to the number of observations (_n_ = 33), it is possible that the surviving regressors are highly collinear, which would suggest that all connections in the brain contain similar information and are similarly related to the rate of forgetting. To exclude this hypothesis, the reduced model was examined using the Variance Inflation Factors (VIF) metric as implemented in R’s car package (Fox & Weisberg, 2018). As a rule of thumb, VIF factors > 10 indicate collinearity (Fox & Monette, 1992). All of the predictors had a VIF value < 4, which suggest that they contributed independently to an individual rate of forgetting. 

```{r}
Xmod <- X[,selected]
vif <- vif(lm(Y ~ ., data = as.data.frame(Xmod)))
ggplot(tibble(vif = vif, regressor=brain$connections), 
       aes(x=regressor, y=vif)) +
  geom_bar(stat="identity", fill="darkgrey") +
  ggtitle("Variance Inflation Factors of the Model") +
  ylab("VIF") +
  ylim(0, 4.5) +
  xlab("Predictors") +
  theme_minimal() +
  geom_label(aes(label=round(vif, 2)), 
             nudge_y=0.3, 
             size=3) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

ggsave("figures/vif.png")
```

### Visualization of the Connectivity

TO visualize the connectivity, we need to create a different tibble.

```{r}
again <- outer(networks[15:17], 
               c(networks, NA), 
               function(a, b) {paste(a, "to", b)})

again <- as.vector(again)
seg <- tibble(connection = again,
              from = substring(again, 1, 9),
              region = substring(again, 14),
              beta = rep(0, 54))

seg$beta[seg$connection %in% brain$connections] <- brain$coef
seg$region[seg$region == "NA"] <- NA
seg <- seg %>%
  mutate(beta = if_else(from == region, NA, beta))

seg <- seg %>%
  mutate(beta = if_else(is.na(region), 0, beta))

```

And now, visualization:

```{r, fig.width=7, fig.height=5}
library(ggseg)
library(ggsegYeo2011)

maxBeta <- max(abs(seg$beta), na.rm = T)

ggplot(seg, aes(fill=beta)) +
  geom_brain(atlas = ggsegYeo2011::yeo17, 
             position = position_brain("horizontal"),
             color="grey") +
  scale_fill_gradient2(#limits=c(-maxBeta, maxBeta), 
                       low = "royalblue", 
                       high = "tomato2", 
                       mid = "white",
                       na.value="grey25") +
  labs(fill = expression(beta)) +
  facet_wrap(~from, ncol=1, nrow = 3,
             labeller = labeller(from = ~paste("With", .x))) +
  ggtitle("Network Connectivity Predictive of SoF") +
  theme_void() +
  theme(legend.position = "bottom") +
  theme(legend.key.width = unit(1.5, "cm"))

ggsave("figures/predictive-connectivity.png")
```

### Network Importance Within DMN 

The resulting connectivity matrix can also be analyzed in terms of network-level measures, such as the importance of a region within the brain. The importance of a region can be calculated as the L1 norm of all of its connections to other networks. The importance of each DMN subnetwork region is visualized below This visualization highlights the prominence of DMN subnetwork B. This network includes the hippocampus and parahippocampal cortices, which are traditionally considered the earlier trace encoding hubs in the memory network. 

```{r, fig.width=7, fig.height=1.8}

importance <- seg %>% 
  group_by(from) %>%
  summarize(importance = sum(abs(beta), na.rm=T)) %>%
  mutate(region = from)

ggplot(importance, aes(fill=importance)) +
  geom_brain(atlas = ggsegYeo2011::yeo17, 
             position = position_brain("horizontal"),
             color="grey") +
  scale_fill_viridis_c(option="plasma", na.value = "white") +
  labs(fill = "Importance") +
  ggtitle("DMN Subnetwork Importance In Predicting SoF") +
  theme_void() +
  theme(legend.position = "bottom",
        legend.key.width = unit(1.5, "cm"))

ggsave("figures/dmn-importance.png")

```

### Network Importance Outside DMN

And now, network importance

```{r, fig.width=7, fig.height=1.8}

importance <- seg %>% 
  group_by(region) %>%
  summarize(importance = sum(abs(beta), na.rm=T)) %>%
  filter(importance > 0)

ggplot(importance, aes(fill=importance)) +
  geom_brain(atlas = ggsegYeo2011::yeo17, 
             position = position_brain("horizontal"),
             color="grey") +
  # scale_fill_gradient2(#limits=c(-maxBeta, maxBeta), 
  #                      low = "royalblue", 
  #                      high = "tomato2", 
  #                      mid = "white",
  #                      na.value="white") +
  #scale_fill_distiller(type="seq", palette="OrRd", 
  #                     na.value="white", limits=c(0, 0.2)) +
  scale_fill_viridis_c(option="plasma", na.value="white") +
  #scale_fill_distiller(type="seq", palette="YlOrRd", 
  #                     na.value="white", limits=c(min(importance$importance), 
  #                                                max(importance$importance) + 0.01)) +
  labs(fill = "Importance") +
  ggtitle("Importance of non-DMN Networks In Predicting SoF") +
  theme_void() +
  theme(legend.position = "bottom",
        legend.key.width = unit(1.5, "cm"))

ggsave("figures/non-dmn-importance.png")
```

## Associative vs Control

```{r, fig.width=4.5, fig.height=3}

brainstuff <- tibble(connections = restricted,
                     from = substring(restricted, 1, 9),
                     region = substring(restricted, 14))

braintype <- full_join(brain, brainstuff) %>%
  mutate(beta = if_else(is.na(coef), 0, coef))

braintype %<>% mutate(Type = "Storage") %>%
  mutate(Type = if_else(region %in% c("Control A",
                                      "Control B",
                                      "Limbic B",
                                      "Salience / Ventral Attention B"),
                        "Retrieval", 
                        Type)) %>%
  filter(!region %in% c("Default A", "Default B", "Default C"))

final <- braintype %>%
  filter(!is.na(coef)) %>%
  #filter(!region %in% c("Salience / Ventral Attention A")) %>%
  group_by(region, Type) %>%
  summarize(importance = sum(abs(beta)))

ggplot(final, aes(y=Type, x=importance, col=Type, fill=Type)) +
  geom_boxplot(aes(y=Type, x=importance, col=Type),
               alpha=0.5, width=0.2) +
  geom_point(size=3, alpha=0.5, col="black") +
  geom_label_repel(aes(label=region),
                   size=2, fill="white") +
  xlab("Connectivity Importance") +
  ylab("Putative Memory Function") +
  ggtitle("Network Importance by Function") +
  theme_minimal() +
  scale_color_aaas() +
  scale_fill_aaas() +
  theme(legend.position = "none")
```

Is the difference between the two groups significant? We can run a T test.

```{r}
t.test(importance ~ Type, data=final) %>%
  tidy() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

And, just to be sure, a Wilcoxon Test:

```{r}
wilcox.test(importance ~ Type, data=final) %>%
  tidy() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

